---
title: "Stability properties for MAR(1) models"
output: pdf_document
fontsize: 10pt
geometry: margin=1.25in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\begin{center}
\textbf{Mark D. Scheuerell} \\
\text{Northwest Fisheries Science Center} \\
\text{National Oceanic and Atmospheric Administration} \\
\text{Seattle, WA USA} \\
\text{mark.scheuerell@noaa.gov}
\end{center}

# Background

There is growing interest in the use of first-order vector autoregressive, or VAR(1), models in ecology where they are often referred to as multivariate autoregressive, or MAR(1), models ($e.g.$, Ives $et \ al$. 2003 $Ecol \ Monographs$ 73:301â€“330).

Assume a MAR(1) model of the general form

$$
\mathbf{x}_t = \mathbf{a} + \mathbf{B} (\mathbf{x}_{t-1} - \mathbf{a}) + \mathbf{w}_t
$$

where $\mathbf{x}_t$ is an $n \times 1$ vector of state variates at time $t$, $\mathbf{a}$ is an $n \times 1$ vector of underlying levels (means) for each of the states, $\mathbf{B}$ is an $n \times n$ interaction matrix, and $\mathbf{w}_t$ is an $n \times 1$ vector of multivariate normal process errors; $\mathbf{w}_t \sim \text{MVN}(\mathbf{0}, \mathbf{Q})$.

Of particular interest here is that the variance-covariance matrix of the stationary distribution for $\mathbf{x}_t$ as $t \rightarrow \infty$ gives an indication of the relative stability of the system. 

## State-space form

This MAR(1) model can be used within a state-space framework, wherein $\mathbf{a}$ is instead included as part of a second model for the observed data $\mathbf{y}$, such that

\begin{gather*}
\mathbf{x}_t = \mathbf{B} \mathbf{x}_{t-1} + \mathbf{w}_t, \\
\mathbf{y}_t = \mathbf{x}_t + \mathbf{a} + \mathbf{v}_t,
\end{gather*}

and $\mathbf{v}_t$ is an $n \times 1$ vector of observation errors, the statistical distribution of which does not affect the variance of the stationary distribution.

### Non-zero mean in the observation

As defined here, the mean of both $\mathbf{x}$ and $\mathbf{y}$ is $\mathbf{0}$ for all $t$. If instead the mean of $\mathbf{y}$ was non-zero, we could simply subtract the mean of the observations $a \ priori$. Thus, if we define $\mathbf{a}$ to be an $n \times 1$ vector wherein the $i$-th element $a_i$ is the mean of $y_i$, and $\mathbf{y}^*_t = \mathbf{y}_t - \mathbf{a}$, then the observation model becomes

\begin{align}
  \mathbf{y}^*_t &= \mathbf{x}_t + \mathbf{v}_t \nonumber \\
  \mathbf{y}_t - \mathbf{a} &= \mathbf{x}_t + \mathbf{v}_t \nonumber \\
  \mathbf{y}_t &= \mathbf{x}_t + \mathbf{a} + \mathbf{v}_t \label{eq4}
\end{align}

When Eqn \eqref{eq1} is combined with Eqn \eqref{eq4}, the full state-space model becomes

\begin{equation} \label{eq5}
  \begin{gathered}
    \mathbf{x}_t = \mathbf{B} \mathbf{x}_{t-1} + \mathbf{w}_t \\
    \mathbf{y}_t = \mathbf{x}_t + \mathbf{a} + \mathbf{v}_t.
  \end{gathered}
\end{equation}

### Non-zero mean in the state

Alternatively, we could instead specify the mean in the state model, such that if $\mathbf{a}$ is an $n \times 1$ vector wherein the $i$-th element $a_i$ is the mean of $x_i$, and $\mathbf{x}^*_t = \mathbf{x}_t - \mathbf{a}$, then the state model would be

\begin{align}
  \mathbf{x}^*_t &= \mathbf{B} \mathbf{x}^*_{t-1} + \mathbf{w}_t \nonumber \\
  \mathbf{x}_t - \mathbf{a} &= \mathbf{B} (\mathbf{x}_{t-1} - \mathbf{a}) + \mathbf{w}_t \nonumber \\
  \mathbf{x}_t &= \mathbf{a} + \mathbf{B} (\mathbf{x}_{t-1} - \mathbf{a}) + \mathbf{w}_t \label{eq6}
\end{align}

Combining Eqn \eqref{eq6} and Eqn \eqref{eq2} leads to this state-space model:

\begin{equation} \label{eq7}
  \begin{gathered}
    \mathbf{x}_t = \mathbf{a} + \mathbf{B} (\mathbf{x}_{t-1} - \mathbf{a}) + \mathbf{w}_t \\
    \mathbf{y}_t = \mathbf{x}_t + \mathbf{v}_t.
  \end{gathered}
\end{equation}

Importantly, both of the specifications of the MARSS models in Eqns \eqref{eq5} and \eqref{eq7} are equivalent, such that we would recover the same parameter values when estimated from the same data.

## Effects of covariates

Often we would like to examine the potential effects of external drivers (covariates) on community dynamics ($e.g.$, temperature effects on poikilotherms), 

### Covariates in the state I

Traditionally, a MARSS model with covariate effects on the state is defined as

\begin{equation} \label{eq8}
  \begin{gathered}
    \mathbf{x}_t = \mathbf{B} \mathbf{x}_{t-1} + \mathbf{C} \mathbf{c}_t + \mathbf{w}_t \\
    \mathbf{y}_t = \mathbf{x}_t + \mathbf{v}_t
  \end{gathered}
\end{equation}

where $\mathbf{C}$ is an $n \times p$ matrix of covariate effects, and $\mathbf{c}_t$ is a $p \times 1$ vector of mean-zero covariates at time $t$.

However, when the state part of MARSS model is defined this way, the estimated covariate effects in $\hat{\mathbf{C}}$ will not reflect the true relationship between $\mathbf{x}$ and $\mathbf{c}$. That is, if we regressed covariate $c_j$ against $x_i$, the absolute value of the estimated slope of the relationship would be larger than that for the $ij$-th element of $\hat{\mathbf{C}}$.

We can demonstrate this phenomenon easily by obtaining realizations of the state model in Eqn \eqref{eq8}. To keep things relatively simple, we will simulate interactions between wolves and moose, and assume there is one covariate ($e.g.$, temperature) that has positive, but different effects on each species. We will use a sine wave as a dummy covariate. Here is our process model

\begin{equation} \label{eq9}
 \begin{bmatrix}
    x_{wolves} \\
    x_{moose} \end{bmatrix}_t = 
 \begin{bmatrix}
     0.6&0.2 \\
    -0.1&0.8 \end{bmatrix} 
 \begin{bmatrix}
    x_{wolves} \\
    x_{moose} \end{bmatrix}_{t-1} +
 \begin{bmatrix}
    0.1 \\
    0.3 \end{bmatrix} 
 \begin{bmatrix}
    c \end{bmatrix}_{t-1} +
 \begin{bmatrix}
    w_{wolves} \\
    w_{moose} \end{bmatrix}_t. \\
\end{equation}

We will further assume the process errors $\mathbf{w}_t$ have different variances and zero covariance, such that. 

\begin{equation} \label{eq10}
\begin{bmatrix}
    w_{wolves} \\
    w_{moose} \end{bmatrix}_t
\sim \text{MVN} \begin{pmatrix}
 \begin{bmatrix}
    0 \\
    0 \end{bmatrix},
 \begin{bmatrix}
    0.03&0 \\
    0&0.15 \\
    \end{bmatrix}
  \end{pmatrix}.
\end{equation}

We being by simulating 100 time steps from the model and plotting the time series for wolves (gray) and moose (brown).

\vspace{0.25in}

```{r sim_mar_cov_1, fig.height=4, fig.width=6}
library(MASS)
set.seed(123)
## time steps
TT <- 200
## interaction matrix
BB <- matrix(c(0.6,-0.1,0.2,0.8),2,2)
## covariate effects
CC <- matrix(c(0.1,0.3),2,1)
## dummy sinusoidal covariate
cc <- matrix(sin(2*pi*seq(TT)/50),1,TT)
## process variance
QQ <- matrix(c(0.15,0,0,0.03),2,2)
## process errors
xx <- ww <- t(mvrnorm(TT, matrix(c(0,0),2,1), QQ))
## simulate process
for(t in 2:TT) {
  xx[,t] <- BB %*% xx[,t-1] + CC %*% cc[,t,drop=FALSE] + ww[,t]
}
## plot states
par(mai=c(0.8,0.8,0.2,0.2), omi=c(0.5,0.1,0.5,0.1))
plot.ts(xx[1,], ylab="Wolves or Moose", col="darkgray", ylim=c(min(xx), max(xx)), lwd=2)
lines(xx[2,], col="brown", lwd=2)
```

\vspace{0.25in}

Now we can examine the relationship between $\mathbf{x}$ and $\mathbf{c}$.

\vspace{0.25in}

```{r res_sim_1, fig.height=4, fig.width=6}
## slopes
sw <- coef(lm(xx[1,] ~ cc[1,] - 1))
sm <- coef(lm(xx[2,] ~ cc[1,] - 1))
## plot covariate vs states
par(mfrow=c(1,2), mai=c(0.8,0.8,0.2,0.2), omi=c(0.5,0.1,0.5,0.1))
plot(cc, xx[1,], pch=16, ylab="Wolves", xlab="Covariate", col="darkgray")
abline(a=0, b=sw)
text(-1, max(xx[1,]), substitute(paste("slope = ", s), list(s = round(sw, 2))),
     adj=c(0,1), cex=0.8)
plot(cc, xx[2,], pch=16, ylab="Moose", xlab="Covariate", col="brown")
abline(a=0, b=sm)
text(-1, max(xx[2,]), substitute(paste("slope = ", s), list(s = round(sm, 2))),
     adj=c(0,1), cex=0.8)
```

\vspace{0.25in}

From these results we can clearly see that the explected relationships between the states and covariate do not hold and that they are indeed biased high.

### Covariates in the state II

Based on the formulation of the state model in Eqn \eqref{eq6}, we can instead replace the specific mean vector $\mathbf{a}$ with a general term for the expectation of $\mathbf{x}$ at time $t$, $\text{E}(\mathbf{x}_t)$, such that

\begin{equation} \label{eq11}
  \mathbf{x}_t = \text{E}(\mathbf{x}_t) + \mathbf{B} (\mathbf{x}_{t-1} - \text{E}(\mathbf{x}_{t-1})) + \mathbf{w}_t.
\end{equation}

At this point we are making no assumptions about the underlying factors that contribute to $\text{E}(\mathbf{x}_t)$, in that there could be some underlying mean, trend, or periodicity in the state. Thus, in a model with covariate effects on the state, we would write the state-space model as

\begin{equation} \label{eq12}
  \begin{gathered}
    \mathbf{x}_t = \mathbf{C} \mathbf{c}_t + \mathbf{B} (\mathbf{x}_{t-1} - \mathbf{C} \mathbf{c}_{t-1}) + \mathbf{w}_t \\
    \mathbf{y}_t = \mathbf{x}_t + \mathbf{v}_t.
  \end{gathered}
\end{equation}

We can now simulate from Eqn \eqref{eq12} and again examine the relationship between $\mathbf{x}$ and $\mathbf{c}$ under this new formulation.

\vspace{0.25in}

```{r sim_mar_cov_2, fig.height=4, fig.width=6}
## simulate process
for(t in 2:TT) {
  xx[,t] <- BB %*% (xx[,t-1] - CC %*% cc[,t-1]) + CC %*% cc[,t] + ww[,t]
}
## plot states
par(mai=c(0.8,0.8,0.2,0.2), omi=c(0.5,0.1,0.5,0.1))
plot.ts(xx[1,], ylab="Wolves or Moose", col="darkgray", ylim=c(min(xx), max(xx)), lwd=2)
lines(xx[2,], col="brown", lwd=2)
```

\vspace{0.25in}

And now we can again examine the estimated relationship between $\mathbf{c}$ and $\mathbf{x}$.

\vspace{0.25in}

```{r res_sim2, fig.height=4, fig.width=6}
## slopes
sw <- coef(lm(xx[1,] ~ cc[1,] - 1))
sm <- coef(lm(xx[2,] ~ cc[1,] - 1))
## plot covariate vs states
par(mfrow=c(1,2), mai=c(0.8,0.8,0.2,0.2), omi=c(0.5,0.1,0.5,0.1))
plot(cc, xx[1,], pch=16, ylab="Wolves", xlab="Covariate", col="darkgray")
abline(a=0, b=sw)
text(-1, max(xx[1,]), substitute(paste("slope = ", s), list(s = round(sw, 2))),
     adj=c(0,1), cex=0.8)
plot(cc, xx[2,], pch=16, ylab="Moose", xlab="Covariate", col="brown")
abline(a=0, b=sm)
text(-1, max(xx[2,]), substitute(paste("slope = ", s), list(s = round(sm, 2))),
     adj=c(0,1), cex=0.8)
```

\vspace{0.25in}

Here we can see that the estimated relationships betweem the covariate and the states are much closer to their true values.

### Covariates in the observation

We saw previously how to include a non-zero mean in the observation model, which was equivalent to doing so in the state model. The same reasoning applies here with respect to covariates in the observation. Using the relationship above in Eqn \eqref{eq4}, we can write the state-space model as

\begin{equation} \label{eq13}
  \begin{gathered}
    \mathbf{x}_t = \mathbf{B} \mathbf{x}_{t-1} + \mathbf{w}_t \\
    \mathbf{y}_t = \mathbf{x}_t + \mathbf{C} \mathbf{c}_t + \mathbf{v}_t.
  \end{gathered}
\end{equation}

As before, we can simulate some data from this model and examine the relationships between $\mathbf{c}$ and the observed data $\mathbf{y}$. To keep the comparison the same as before, we will assume that there is no obsveration error ($i.e.$, $\mathbf{v}_t = 0 \ \forall \ t$).

\vspace{0.25in}

```{r sim_mar_cov_obs, fig.height=4, fig.width=6}
## initialize y
yy <- xx
## simulate data
for(t in 2:TT) {
  xx[,t] <- BB %*% xx[,t-1] + ww[,t]
  yy[,t] <- xx[,t] + CC %*% cc[,t]
}
## plot states
par(mai=c(0.8,0.8,0.2,0.2), omi=c(0.5,0.1,0.5,0.1))
plot.ts(yy[1,], ylab="Wolves or Moose", col="darkgray", ylim=c(min(yy), max(yy)), lwd=2)
lines(yy[2,], col="brown", lwd=2)
```

\vspace{0.25in}

And lastly we can examine the estimated relationship between $\mathbf{c}$ and $\mathbf{y}$.

\vspace{0.25in}

```{r res_sim_obs, fig.height=4, fig.width=6}
## slopes
sw <- coef(lm(yy[1,] ~ cc[1,] - 1))
sm <- coef(lm(yy[2,] ~ cc[1,] - 1))
## plot covariate vs states
par(mfrow=c(1,2), mai=c(0.8,0.8,0.2,0.2), omi=c(0.5,0.1,0.5,0.1))
plot(cc, yy[1,], pch=16, ylab="Wolves", xlab="Covariate", col="darkgray")
abline(a=0, b=sw)
text(-1, max(yy[1,]), substitute(paste("slope = ", s), list(s = round(sw, 2))),
     adj=c(0,1), cex=0.8)
plot(cc, yy[2,], pch=16, ylab="Moose", xlab="Covariate", col="brown")
abline(a=0, b=sm)
text(-1, max(yy[2,]), substitute(paste("slope = ", s), list(s = round(sm, 2))),
     adj=c(0,1), cex=0.8)
```

\vspace{0.25in}

Again, the estimated relationships are the same as before.

# Variance of the stationary distribution

We will restrict this treatment to stationary models wherein all of the eigenvalues of $\mathbf{B}$ lie within the unit circle. Because $t = (t-1)$ as $t \rightarrow \infty$, under assumptions of stationarity we can write the process equation from the above state-space model as

$$
\mathbf{x}_t = \mathbf{B} \mathbf{x}_t + \mathbf{w}_t.
$$

From this, it follows that

$$
\rm Var(\mathbf{x}_t) = \mathbf{B} Var(\mathbf{x}_t) \mathbf{B}^\top + Var(\mathbf{w}_t).
$$

If we define $\mathbf{\Sigma} = \rm Var(\mathbf{x}_t)$, then

$$  
\mathbf{\Sigma} = \mathbf{B} \mathbf{\Sigma} \mathbf{B}^\top + \mathbf{Q}.
$$

Unfortunately, however, there is no closed-form solution for $\mathbf{\Sigma}$ when written in this form.

## The $vec$ operator

It turns out that we can use the $vec$ operator to derive an explicit solution for $\mathbf{\Sigma}$. The $vec$ operator converts an $i \times j$ matrix into an $(ij) \times 1$ column vector. For example, if

$$
\mathbf{M} = 
\begin{bmatrix}
    1 & 3 \\
    2 & 4
\end{bmatrix},
$$

then

$$
vec(\mathbf{M}) = 
\begin{bmatrix}
    1 \\
    2 \\
    3 \\
    4 
\end{bmatrix}.
$$

## Solution

Thus, if $\mathbf{I}$ is an $n \times n$ identity matrix, and we define $\mathcal{I} = (\mathbf{I} \otimes \mathbf{I})$ and $\mathcal{B} = (\mathbf{B} \otimes \mathbf{B})$, then

$$
vec(\mathbf{\Sigma}) = (\mathcal{I} - \mathcal{B})^{-1} vec(\mathbf{Q}).
$$

# Importance of species interactions to stability

Among the many ways of classifying stability, I am interested in the extent to which community interactions, relative to environmental forcing, contribute to the overall variance of the stationary distribution. In a stable system, any perturbation affecting one or more of the community members does not amplify as it moves throughout the community as a whole, such that the variances in log-densities over time would be driven almost entirely by random environmental variation. Given that same magnitude of random environmental variation, less stable systems are characterized by greater variances in the temporal dynamics of their constituents.

Here we will use determinants to measure the "volume" of a matrix. Looking back to the matrix form of the equation for the variance of the stationary distribution, we see that

$$
\mathbf{\Sigma} = \mathbf{B} \mathbf{\Sigma} \mathbf{B}^\top + \mathbf{Q}, \\
$$

and hence

$$
\mathbf{\Sigma} - \mathbf{Q} = \mathbf{B} \mathbf{\Sigma} \mathbf{B}^\top.
$$

Therefore, the volume of the difference $\mathbf{\Sigma} - \mathbf{Q}$ provides a measure of how much species interactions contribute to the variance of the stationary distribution. Taking determinants of both sides, we get

\begin{align*}
\text{det}(\mathbf{\Sigma} - \mathbf{Q}) &= \text{det}(\mathbf{B} \mathbf{\Sigma} \mathbf{B}^\top) \\
 &= \text{det}(\mathbf{B}) \text{det}(\mathbf{\Sigma}) \text{det}(\mathbf{B}^\top) \\
 &= \text{det}(\mathbf{B}) \text{det}(\mathbf{\Sigma}) \text{det}(\mathbf{B}).  
\end{align*}

The proportion $\pi_\mathbf{B}$ of the volume of $\mathbf{\Sigma}$ attributable to species interactions is then

$$
\pi_\mathbf{B}  = \frac{\text{det}(\mathbf{\Sigma} - \mathbf{Q})}{\text{det}(\mathbf{\Sigma})} = \text{det}(\mathbf{B})^2.
$$

# The effects of covariates

Often we would like to examine the potential effects of covariates on community dynamics. In that case, the MARSS(1) model becomes

\begin{gather*}
\mathbf{x}_t = \mathbf{B} \mathbf{x}_{t-1} + \mathbf{C} \mathbf{c}_t + \mathbf{w}_t, \\
\mathbf{y}_t = \mathbf{x}_t + \mathbf{a} + \mathbf{v}_t,
\end{gather*}

where $\mathbf{C}$ is an $n \times p$ matrix of covariate effects, and $\mathbf{c}_t$ is a $p \times 1$ vector of covariates at time $t$. Although the variance of the stationary distrbution will be affected by any covariate effects, the method described above for estimating the relative effects of species interactions is unaffected. That is, if $\mathbf{\Xi}$ is the variance-covariance matrix of the covariates in $\mathbf{c}$, then analogous to above the variance of the stationary distrbution will be

\begin{align*}
\mathbf{\Sigma} &= \mathbf{B} \mathbf{\Sigma} \mathbf{B}^\top + \mathbf{C} \mathbf{\Xi} \mathbf{C}^\top + \mathbf{Q}, \\
\mathbf{\Sigma - \mathbf{C} \mathbf{\Xi} \mathbf{C}^\top - \mathbf{Q}} &= \mathbf{B} \mathbf{\Sigma} \mathbf{B}^\top ,
\end{align*}

and hence the proportion $\pi_\mathbf{B}$ of the volume of $\mathbf{\Sigma}$ attributable to species interactions is, as before, given by

$$
\pi_\mathbf{B}  = \frac{\text{det}(\mathbf{\Sigma} - \mathbf{C} \mathbf{\Xi} \mathbf{C}^\top - \mathbf{Q})}{\text{det}(\mathbf{\Sigma})} = \text{det}(\mathbf{B})^2.
$$

# Long-term changes in abundance

Ives $et \ al$. ($Ecology$ 1999 80:1405â€“1421) provide a means for assessing the expected long-term change in the density (biomass) of species $i$, $L_i$, within a community of $p$ total members, owing to the effect of some covariate $j$. Specifically,

$$
L_i = \frac{\text{det}(\mathbf{B}_1, ..., \mathbf{B}_{i-1}, \mathbf{C}_j, \mathbf{B}_{i+1}, ..., \mathbf{B}_p)}{\text{det}(\mathbf{B}_1, ..., \mathbf{B}_p)},
$$

where $\mathbf{B}_i$ is a $p \times 1$ column vector containing the estimated effects of species $i$ on all of the species (including itself), and $\mathbf{C}_i$ is a $p \times 1$ column vector containing the estimated effects of covariate $j$ on each of the species.

# Reactivity

We can calculate the reactivity of a community following an external perturbation. There are two methods to do so:

1. $= \displaystyle -\frac{\text{tr}(\mathbf{Q})}{\text{tr}(\mathbf{\Sigma})} \leq 1-\lambda_{max}(\mathbf{B}^\top \mathbf{B})$ (Ives $et \ al$. 2003); and

2. $= \log ||\mathbf{B}||_2 = \log \sqrt{\lambda_{max}(\mathbf{B}^\top \mathbf{B})} = \log \ \sigma_{max} (\mathbf{B})$ (Neubert $et \ al$. 2009),

where $||\cdot||_2$ is the spectral norm, $\lambda_{max}(\cdot)$ is the maximum eigenvalue, and $\sigma_{max}(\cdot)$ is the largest singular value.

Note that method #1 requires the estimates of the process covariance $\mathbf{Q}$ and stationary covariance $\mathbf{\Sigma}$, whereas method #2 only requires estimates of $\mathbf{B}$.

# Effect of density-dependence

In a MAR(1) model of community dynamics, the matrix $\mathbf{B}$ maps the vector of log-densities from one time step to another. In particular, the diagonal elements of $\mathbf{B}$ control the degree of density-dependence via the degree of so-called mean reversion. Because we are interested in stationary dynamics, the diagonals of $\mathbf{B}$ will all be less than 1 in absolute value. As such, here are a few generalizations:

1. As $|b_{ii}| \rightarrow 0$ the strength of density dependence increases ($i.e.$, the degree of mean reversion increases); and

2. As $b_{ii} \rightarrow 1$ the strength of density dependence decreases to the point where there is no density dependence when $b_{ii} = 1$ ($i.e.$, the temporal dynamics become a non-stationary random walk).

Furthermore, the determinant (and the trace) of a matrix are functions of the eigenvalues $\lambda_i$, such that for an $N \times N$ matrix $\mathbf{B}$:

$$
\text{det}(\mathbf{B}) = \prod_{i=1}^N \lambda_i.
$$

The eigenvalues themselves are a function of all of the elements in $\mathbf{B}$, but they are particularly sensitive to the elements along the diagonal. We can formally examine the sensitivity of $\text{det}(\mathbf{B})$ to each of the elements in $\mathbf{B}$ via the following relationship:

$$
\frac{\partial \text{det}(\mathbf{B})}{\partial \mathbf{B}_{ij}} = \text{det}(\mathbf{B}) (\mathbf{B}^{-1})_{ji}.
$$

Thus, for any matrix $\mathbf{B}$ we can ask whether its determinant is most sensitive to a diagonal versus an off-diagonal element.

## A simulation study

Let's examine the determinant and the location of its maximum partial derivative for whole bunch of matrices. We'll do so over a range of matrix sizes and entries. Here's the pseudo code:

* Choose a matrix size

* Do the following many times

     + Populate the entire matrix with random values over a specified range

     + Replace the diagonal with random values over a specified range

     + Calculate partial derivatives and note location of max in abs value

     + For the same matrix, increase the range of values in the diagonal

     + Calculate partial derivatives and note location of max in abs value
     
     + Calculate det(**B**)^2^  

And here is the corresponding **R** code.

```{r ddet, fig.height=8, cache=TRUE}
## maximum matrix dim
pp <- 10
## number of samples
mc <- 10000
## maximum diagonal value (x10)
dd <- 10
## min off-diagonal
dn <- -0.5
## min off-diagonal
dx <- 0.5
## sample over matrix size and diagonal
sam <- dtm <- matrix(NA,mc,dd-1)
res1 <- res2 <- matrix(NA,dd-1,pp-1)
for(ii in 2:pp) {
  for(jj in 1:mc) {
    mat <- matrix(runif(ii^2,dn,dx),ii,ii)
    for(kk in 2:dd) {
      tmp <- mat
      diag(tmp) <- runif(ii,kk-1,kk)/10
      rc <- which(abs(t(solve(tmp)))==max(abs(t(solve(tmp)))))
      sam[jj,kk-1] <- rc %in% seq(1,ii^2,ii+1)
      dtm[jj,kk-1] <- det(tmp)^2
    }
  }
  res1[,ii-1] <- apply(sam, 2, sum)/mc
  res2[,ii-1] <- apply(dtm, 2, mean)
}
## plot the results
par(mfrow=c(2,1), mai=c(0.9,0.9,0.1,0.1), omi=rep(0,4))
matplot(res1, type="l", lty="solid", col=rainbow(pp-1, start=0.7, end=1),
        ylim=c(0,1), las=1,
        xaxt="n", ylab="Prop. of maxima along diagonal", xlab="Range of diagonal")
axis(1, at=seq(pp-1), cex.axis=0.8, labels=c("0.1-0.2", "0.2-0.3", "0.3-0.4",
                                             "0.4-0.5", "0.5-0.6", "0.6-0.7",
                                             "0.7-0.8", "0.8-0.9", "0.9-1.0"))
legend(x="topleft", legend = seq(2,pp), col=rainbow(pp-1, start=0.7, end=1),
       lty="solid", bty="n", title="Matrix dim", cex=0.8, inset=c(0.03,0.01))
matplot(res2, type="l", lty="solid", col=rainbow(pp-1, start=0.7, end=1),
        las=1,
        xaxt="n", ylab=expression(det(bold(B))^2), xlab="Range of diagonal")
axis(1, at=seq(pp-1), cex.axis=0.8, labels=c("0.1-0.2", "0.2-0.3", "0.3-0.4",
                                             "0.4-0.5", "0.5-0.6", "0.6-0.7",
                                             "0.7-0.8", "0.8-0.9", "0.9-1.0"))
legend(x="topleft", legend = seq(2,pp), col=rainbow(pp-1, start=0.7, end=1),
       lty="solid", bty="n", title="Matrix dim", cex=0.8, inset=c(0.03,0.01))
```

# Example 1

As an example, I will fit a MARSS(1) model to some plankton data from Lake Washington (USA). These data were made publicly available by Dr. Daniel Schindler of the University of Washington, and they have been included as part of the `MARSS` package for \textbf{R}. To speed up model fitting, I will focus on a representative time period from 1981 through 1985, and only use a subset of the plankton groups.

```{r load_data}
library(MARSS)
## load the data (2 datasets here)
data(lakeWAplankton)
raw <- as.data.frame(lakeWAplanktonRaw)
## keep only 1981-85
dat <- subset(raw, Year>=1981 & Year<=1985)
## contents
colnames(dat)
## phytoplankton names (omitting bluegreens)
phyto_names <- c("Cryptomonas", "Diatoms", "Greens")
## zooplankton names (omitting Leptodora & Neomysis)
zoops_names <- c("Cyclops", "Diaptomus",   # copepods
                 "Daphnia", "Non.daphnid.cladocerans") # cladocerans
all_names <- c(phyto_names,zoops_names)
## number of taxa
nP <- length(phyto_names)
nZ <- length(zoops_names)
nn <- nP + nZ 
## length of ts
TT <- dim(dat)[1]
```

In addition to the biotic interactions, I will include the effects of water temperature on the plankton.

## Model definition

Now we need to set up the various vectors and matrices that define our MARSS(1) model. Here I will make the following assumptions:

1. independent and identically distributed observation errors for all of the taxa within each of the two plankton groups;
2. a block-diagonal form for $\mathbf{Q}$ wherein all of the taxa within a plankton group have the same variance, and no covariance within and among blocks; and
3. fixed monthly effects to eliminate seasonal components.

The following code also makes use of various shorthand character description of vectors and matrics in `MARSS`.

```{r setup_marss}
mod_list <- list()
## process model
mod_list$B <- "unconstrained"
mod_list$U <- "zero"
## include seasonal effect
mod_list$C <- "unconstrained"
cc <- subset(as.data.frame(lakeWAplanktonTrans),
             Year>=1980 & Year<=1989, select=Temp)
mod_list$c <- matrix(cc$Temp,1,TT)
## var-cov of process (environmental errors)
mod_list$Q <- "diagonal and unequal"
## observation model
mod_list$Z <- "identity"
## set levels to zero & subtract the mean from the data below
mod_list$A <- "zero"
mod_list$D <- "zero"
mod_list$d <- "zero"
## observation var-cov
mod_list$R <- matrix(list(0), nn, nn)
diag(mod_list$R) <- c(rep("r_phyto", nP), rep("r_zoops", nZ))
## init x
mod_list$tinitx <- 1
```

The model we are fitting is based upon log-densities, so we'll need to transform the data before proceeding. In this case, zeroes in the data set are indicative of the sampling program and do not indicate true absences; therefore I will convert them to NA.

```{r org_data}
yy <- dat[,all_names]
yy[yy==0] <- NA
## log-transform variates
## data must also be n x T, so transpose
yy <- t(log(yy))
## subtract the mean from each ts
yy <- yy - apply(yy, 1, mean, na.rm=TRUE)
matplot(t(yy), type="l")
```

Now we can fit the model. Note that the output from `MARSS` is rather voluminous, so I will supress it here.

```{r fit_mod_1, cache=TRUE}
## fit model
mod <- MARSS(yy, model=mod_list, control=list(maxit=5000, safe=TRUE))
```

And inspect the residuals.

```{r resids_1}
rr <- residuals(mod)$state.residuals
matplot(t(rr), type="l")
```

## Importance of species interactions

Now we can calculate the proportion $\pi_\mathbf{B}$ of the variance of the stationary distribution $\mathbf{\Sigma}$ owing to species interactions.

```{r pi_b_1}
## get B matrix
BB <- coef(mod, type="matrix")$B
round(BB,2)
## calc pi_B
det(BB)^2
## Ives adjustment for other community comparisons
det(BB)^(2/nn)
```

It looks like $\pi_\mathbf{B}$ is very small, which suggests that most of the temporal dynamics are driven by environmental forcing.

## Long-term change in abundance

Next let's calculate the expected long-term changes in the densities of the L WA plankton community owing to the "environment" ($i.e.$, a cosine wave indicative of seasonal variation.

```{r long_term_changes}
CC <- coef(mod, type="matrix")$C
LL <- rep(NA, nn)
for(i in 1:nn) {
  B1 <- BB
  B1[,i] <- CC
  LL[i] <- det(B1)/det(BB)
}
data.frame(taxa=all_names, L=round(LL, 2))
```

## Reactivity

Lastly, we can calculate the reactivity of the community following a perturbation using both of the methods described above. Note that for the first method we'll make use of the `vec` function from the `MARSS` package.

### Ives $et \ al$.

```{r react_ives_1}
tr <- function(x) {
  return(sum(diag(x)))
}
## covariance of proc errors
QQ <- coef(mod, type="matrix")$Q
## covariance of stationary dist
vSigma <- solve(diag(nn) %x% diag(nn) - BB %x% BB) %*% MARSS:::vec(QQ)
Sigma <- matrix(vSigma, nn, nn)
## reactivity
RI <- -tr(QQ) / tr(Sigma)
RI
## worst case
1 - eigen(t(BB) %*% BB)$values[1]
```

### Neubert $et \ al$.

```{r react_neub_1}
## reactivity
RN <- log(norm(BB, "2"))
RN
log(sqrt(eigen(t(BB) %*% BB)$values[1]))
log(max(svd(BB)$d))

eigen(t(BB) %*% BB)$values
```

# Example 2

e analysis, but instead of using temperature as a covariate, I will use a combination of sine and 
Now I will repeat the same analysis, but instead of using temperature as a covariate, I will use a combination of sine and cosine waves to capture the seasonal variations. 

```{r setup_marss_2}
## change seasonal effect
mod_list$C <- "unconstrained"
mod_list$c <- matrix(NA,2,TT)
mod_list$c[1,] <- cos(2*pi*seq(TT)/12)
mod_list$c[2,] <- sin(2*pi*seq(TT)/12)
```

Now we can fit the model.

```{r fit_mod_2, cache=TRUE}
## fit model
mod <- MARSS(yy, model=mod_list, control=list(maxit=5000, safe=TRUE))
```

And inspect the residuals.

```{r, resids_2}
rr <- residuals(mod)$state.residuals
matplot(t(rr), type="l")
```

## Importance of species interactions

Now we can calculate the proportion $\pi_\mathbf{B}$ of the variance of the stationary distribution $\mathbf{\Sigma}$ owing to species interactions.

```{r pi_b_2}
## get B matrix
BB <- coef(mod, type="matrix")$B
round(BB,2)
## calc pi_B
det(BB)^2
## Ives adjustment for other community comparisons
det(BB)^(2/nn)
```

It looks like $\pi_\mathbf{B}$ is very small, which suggests that most of the temporal dynamics are driven by environmental forcing.

## Long-term change in abundance

Next let's calculate the expected long-term changes in the densities of the L WA plankton community owing to the "environment" ($i.e.$, a cosine wave indicative of seasonal variation.

```{r long_term_changes_2}
CC <- coef(mod, type="matrix")$C
LL <- matrix(NA, nn, 2)
for(i in 1:nn) {
  B1 <- B2 <- BB
  B1[,i] <- CC[1, drop=FALSE]
  LL[i,1] <- det(B1)/det(BB)
  B2[,i] <- CC[2, drop=FALSE]
  LL[i,2] <- det(B2)/det(BB)
}
data.frame(taxa=all_names, Lcos=round(LL[,1], 2), Lsin=round(LL[,2], 2))
```

